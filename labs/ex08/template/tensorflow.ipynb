{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 25813229.32875355\n",
      "1 19571001.62322707\n",
      "2 17668517.32639038\n",
      "3 17109035.32524682\n",
      "4 16488888.515739556\n",
      "5 14869960.268008966\n",
      "6 12330493.317577917\n",
      "7 9280933.29513495\n",
      "8 6512520.066185092\n",
      "9 4336956.253707863\n",
      "10 2843972.508909498\n",
      "11 1876167.9066414195\n",
      "12 1274044.4735338099\n",
      "13 899599.7213011393\n",
      "14 663936.5498390435\n",
      "15 510711.7247360612\n",
      "16 407317.4636063746\n",
      "17 334370.21765425\n",
      "18 280619.03146484756\n",
      "19 239395.31940283097\n",
      "20 206719.80228366828\n",
      "21 180092.57377574232\n",
      "22 157988.24056537467\n",
      "23 139344.5653479541\n",
      "24 123427.79484932969\n",
      "25 109721.05919849884\n",
      "26 97851.4359166623\n",
      "27 87515.02558262886\n",
      "28 78463.72575964415\n",
      "29 70505.49955875934\n",
      "30 63481.02326151405\n",
      "31 57269.80280771483\n",
      "32 51761.14142377811\n",
      "33 46862.22464662991\n",
      "34 42496.90910871401\n",
      "35 38596.348369306244\n",
      "36 35103.70505312935\n",
      "37 31972.03166336148\n",
      "38 29157.916964128315\n",
      "39 26625.203595298008\n",
      "40 24342.339322691307\n",
      "41 22280.110136933406\n",
      "42 20415.871356697637\n",
      "43 18729.148588187294\n",
      "44 17198.02092085324\n",
      "45 15807.008741569143\n",
      "46 14542.011859056547\n",
      "47 13389.209302606683\n",
      "48 12337.882558389274\n",
      "49 11377.948362210582\n",
      "50 10500.855342229805\n",
      "51 9698.214946148504\n",
      "52 8963.01029071543\n",
      "53 8288.934725181027\n",
      "54 7670.552449971652\n",
      "55 7102.671128656886\n",
      "56 6580.697642803227\n",
      "57 6100.612095395962\n",
      "58 5658.448635297592\n",
      "59 5251.083725386072\n",
      "60 4875.534820291286\n",
      "61 4529.077943012364\n",
      "62 4209.364274228984\n",
      "63 3913.9048761626673\n",
      "64 3640.801518953188\n",
      "65 3388.250955327974\n",
      "66 3154.5963852137693\n",
      "67 2937.992346257871\n",
      "68 2737.3013960634935\n",
      "69 2551.3241017419114\n",
      "70 2378.853149572752\n",
      "71 2218.82337707628\n",
      "72 2070.26763462533\n",
      "73 1932.3001627904991\n",
      "74 1804.1111754819572\n",
      "75 1684.924725483498\n",
      "76 1574.1187788461195\n",
      "77 1471.051814993104\n",
      "78 1375.1040874570572\n",
      "79 1285.7796331189047\n",
      "80 1202.5842742937396\n",
      "81 1125.0699993045962\n",
      "82 1052.8401705215158\n",
      "83 985.4832878841021\n",
      "84 922.678227055156\n",
      "85 864.084594523094\n",
      "86 809.3954840415946\n",
      "87 758.3585076447346\n",
      "88 710.6841701303315\n",
      "89 666.167758977985\n",
      "90 624.5741140744722\n",
      "91 585.7048721201157\n",
      "92 549.3767018688073\n",
      "93 515.39900082657\n",
      "94 483.62627601769117\n",
      "95 453.9076880988806\n",
      "96 426.08796936822915\n",
      "97 400.057280263935\n",
      "98 375.6835468389947\n",
      "99 352.86375017352464\n",
      "100 331.49989268849805\n",
      "101 311.4933694739884\n",
      "102 292.74571946841803\n",
      "103 275.1719730109529\n",
      "104 258.6982469400888\n",
      "105 243.25338361059067\n",
      "106 228.76692298830056\n",
      "107 215.18109441976526\n",
      "108 202.43361328485798\n",
      "109 190.47050421014274\n",
      "110 179.24475617793607\n",
      "111 168.70597055748624\n",
      "112 158.81176934303417\n",
      "113 149.52145406613369\n",
      "114 140.7946416751099\n",
      "115 132.59731403289578\n",
      "116 124.89472754554102\n",
      "117 117.65605259942083\n",
      "118 110.85433000901085\n",
      "119 104.45987986600011\n",
      "120 98.44894364032729\n",
      "121 92.79642462451561\n",
      "122 87.47936230538133\n",
      "123 82.47846130366315\n",
      "124 77.77349219388401\n",
      "125 73.34619396848603\n",
      "126 69.18076222806539\n",
      "127 65.25976701863263\n",
      "128 61.56931071509566\n",
      "129 58.095501442980414\n",
      "130 54.82465569596065\n",
      "131 51.74333103973501\n",
      "132 48.84095089546305\n",
      "133 46.10669387264531\n",
      "134 43.53094354139098\n",
      "135 41.1041524506757\n",
      "136 38.81734081697293\n",
      "137 36.6614188149166\n",
      "138 34.62257971505756\n",
      "139 32.700940053332054\n",
      "140 30.889513089447213\n",
      "141 29.181512953682113\n",
      "142 27.570899435797816\n",
      "143 26.05233623373858\n",
      "144 24.6197380613622\n",
      "145 23.268277237041918\n",
      "146 21.993584340336525\n",
      "147 20.790841264065982\n",
      "148 19.655738083243975\n",
      "149 18.584403661463796\n",
      "150 17.57327087680966\n",
      "151 16.618668804514407\n",
      "152 15.717570239078466\n",
      "153 14.866624435936748\n",
      "154 14.062987957095883\n",
      "155 13.304130458523193\n",
      "156 12.587521096411272\n",
      "157 11.910587304275737\n",
      "158 11.270957610277499\n",
      "159 10.666584814363445\n",
      "160 10.095741435229291\n",
      "161 9.55679489839249\n",
      "162 9.04751761861366\n",
      "163 8.566124307540631\n",
      "164 8.110964499573715\n",
      "165 7.6806697654208005\n",
      "166 7.273763057840311\n",
      "167 6.888938637344939\n",
      "168 6.524958765802069\n",
      "169 6.180769646633498\n",
      "170 5.855205610994182\n",
      "171 5.547158751067335\n",
      "172 5.255702427817903\n",
      "173 4.979959015583711\n",
      "174 4.719073094481138\n",
      "175 4.472168092757047\n",
      "176 4.238510696080031\n",
      "177 4.017293621200976\n",
      "178 3.8078888427148336\n",
      "179 3.609659610768214\n",
      "180 3.4219842988275806\n",
      "181 3.2442990097089703\n",
      "182 3.076097475129588\n",
      "183 2.9167794553658606\n",
      "184 2.765875132223838\n",
      "185 2.622950211547222\n",
      "186 2.487571573528661\n",
      "187 2.3593399242719886\n",
      "188 2.2378747162514085\n",
      "189 2.1227731890503603\n",
      "190 2.0137199414862277\n",
      "191 1.9103991411827566\n",
      "192 1.8124824070930283\n",
      "193 1.7196818293379363\n",
      "194 1.6317432364556208\n",
      "195 1.5483773193026797\n",
      "196 1.4693521255486424\n",
      "197 1.3944433587163199\n",
      "198 1.3234302352215765\n",
      "199 1.2561092797524158\n",
      "200 1.1922860012858958\n",
      "201 1.1317589281874287\n",
      "202 1.0743610226872318\n",
      "203 1.019921923255423\n",
      "204 0.9682948926129216\n",
      "205 0.9193330032555798\n",
      "206 0.8728924236848692\n",
      "207 0.8288466956703159\n",
      "208 0.7870575823583474\n",
      "209 0.7474126946813504\n",
      "210 0.7097995268538491\n",
      "211 0.6741126577562186\n",
      "212 0.6402482573506794\n",
      "213 0.608118528572104\n",
      "214 0.577624232694123\n",
      "215 0.5486851758160483\n",
      "216 0.5212222400950547\n",
      "217 0.4951602240240802\n",
      "218 0.4704171432624694\n",
      "219 0.4469330496033699\n",
      "220 0.4246384414592908\n",
      "221 0.40347553460221613\n",
      "222 0.38338370210699846\n",
      "223 0.364305951686804\n",
      "224 0.3461920286848068\n",
      "225 0.3289956868514023\n",
      "226 0.3126674745509454\n",
      "227 0.29715912746415557\n",
      "228 0.282430877634196\n",
      "229 0.2684436561991875\n",
      "230 0.2551592223464454\n",
      "231 0.2425416234891547\n",
      "232 0.23055930567312757\n",
      "233 0.21917471784811732\n",
      "234 0.20836239773288878\n",
      "235 0.1980893496250315\n",
      "236 0.18832940992739242\n",
      "237 0.17905797386557032\n",
      "238 0.17024948083310004\n",
      "239 0.1618793558051037\n",
      "240 0.15392592846747316\n",
      "241 0.14636871756434777\n",
      "242 0.1391883048069137\n",
      "243 0.13236438009321988\n",
      "244 0.1258789869922216\n",
      "245 0.11971559970404366\n",
      "246 0.11385755576723688\n",
      "247 0.10828988729213623\n",
      "248 0.10299757316010888\n",
      "249 0.0979668056197457\n",
      "250 0.09318506430269065\n",
      "251 0.08864097760657609\n",
      "252 0.08432028025458382\n",
      "253 0.08021201213280793\n",
      "254 0.07630643492113078\n",
      "255 0.07259302742044137\n",
      "256 0.06906283964889583\n",
      "257 0.06570635561294413\n",
      "258 0.06251460314163806\n",
      "259 0.059479823568819645\n",
      "260 0.05659424994567738\n",
      "261 0.05385001984031633\n",
      "262 0.05123998711892333\n",
      "263 0.04875799021956009\n",
      "264 0.046397496151393214\n",
      "265 0.044152540517862375\n",
      "266 0.042017301572147475\n",
      "267 0.03998638643254257\n",
      "268 0.03805509601510523\n",
      "269 0.036217583445281074\n",
      "270 0.034469993009914886\n",
      "271 0.03280726376768679\n",
      "272 0.031225537856198055\n",
      "273 0.02972085186497124\n",
      "274 0.028289489775410487\n",
      "275 0.026927758749619616\n",
      "276 0.025632419292960527\n",
      "277 0.024399783347413724\n",
      "278 0.023226885397442575\n",
      "279 0.02211096358262758\n",
      "280 0.0210491092883074\n",
      "281 0.020038677353091443\n",
      "282 0.019077333275331038\n",
      "283 0.018162515361820144\n",
      "284 0.017291915413531537\n",
      "285 0.016463562540255168\n",
      "286 0.015675071149846623\n",
      "287 0.014924685214658803\n",
      "288 0.014210577082013431\n",
      "289 0.013530910151862969\n",
      "290 0.01288401040724124\n",
      "291 0.012268293229709303\n",
      "292 0.011682305476757103\n",
      "293 0.011124564711160837\n",
      "294 0.010593642970955646\n",
      "295 0.010088281625816656\n",
      "296 0.009607221187906667\n",
      "297 0.009149226413201489\n",
      "298 0.008713273653247004\n",
      "299 0.008298234741130223\n",
      "300 0.007903137498160705\n",
      "301 0.007527060326291978\n",
      "302 0.007168989431388929\n",
      "303 0.006828035519411477\n",
      "304 0.006503432350116125\n",
      "305 0.006194370851729584\n",
      "306 0.0059001117888748955\n",
      "307 0.005619950573326491\n",
      "308 0.005353178340628958\n",
      "309 0.005099162131377922\n",
      "310 0.004857328532444709\n",
      "311 0.004626993387363185\n",
      "312 0.004407668643007073\n",
      "313 0.004198834851716316\n",
      "314 0.0039999557952525855\n",
      "315 0.003810539057314595\n",
      "316 0.0036301648972547106\n",
      "317 0.0034583808532927707\n",
      "318 0.003294807355935496\n",
      "319 0.0031390229268234548\n",
      "320 0.002990648411339199\n",
      "321 0.0028493229575063134\n",
      "322 0.0027147114409203344\n",
      "323 0.0025865006160837424\n",
      "324 0.002464388200234539\n",
      "325 0.0023480903941260644\n",
      "326 0.002237315406666238\n",
      "327 0.0021318063651282644\n",
      "328 0.002031292603971239\n",
      "329 0.0019355407917950726\n",
      "330 0.0018443337134353662\n",
      "331 0.0017574486695676147\n",
      "332 0.0016746902749179855\n",
      "333 0.0015958422230212027\n",
      "334 0.001520727891760045\n",
      "335 0.0014491926846296804\n",
      "336 0.0013810277183265614\n",
      "337 0.001316085691066032\n",
      "338 0.0012542162270641202\n",
      "339 0.0011952675820955084\n",
      "340 0.0011391103927554922\n",
      "341 0.0010856006399225686\n",
      "342 0.0010346216617243725\n",
      "343 0.0009860571503745221\n",
      "344 0.0009397907688865447\n",
      "345 0.0008956938309489532\n",
      "346 0.0008536765070054902\n",
      "347 0.0008136420976031148\n",
      "348 0.0007754961509784571\n",
      "349 0.0007391502988089877\n",
      "350 0.0007045160816190294\n",
      "351 0.0006715121910682102\n",
      "352 0.0006400703236096972\n",
      "353 0.0006100992120607117\n",
      "354 0.0005815401269919254\n",
      "355 0.0005543267558606475\n",
      "356 0.000528392705405462\n",
      "357 0.000503675727676882\n",
      "358 0.0004801211983246308\n",
      "359 0.00045767471159133667\n",
      "360 0.00043628522058284275\n",
      "361 0.00041590018542926013\n",
      "362 0.00039647020433770995\n",
      "363 0.000377952378064046\n",
      "364 0.0003603024948813678\n",
      "365 0.00034348066335753775\n",
      "366 0.00032744795609955014\n",
      "367 0.00031216998378683384\n",
      "368 0.00029760652099189303\n",
      "369 0.00028372756162865494\n",
      "370 0.00027049601691182205\n",
      "371 0.0002578848914523257\n",
      "372 0.0002458641293638118\n",
      "373 0.00023440694872470168\n",
      "374 0.00022348565452221664\n",
      "375 0.00021307529975459727\n",
      "376 0.0002031514677033642\n",
      "377 0.00019369421623134281\n",
      "378 0.0001846782716198602\n",
      "379 0.0001760832258918803\n",
      "380 0.0001678894983758943\n",
      "381 0.00016007962827544066\n",
      "382 0.00015263384059291212\n",
      "383 0.0001455356722522913\n",
      "384 0.00013876945557450644\n",
      "385 0.00013231980468224542\n",
      "386 0.0001261710297600106\n",
      "387 0.000120308307916066\n",
      "388 0.00011471896625990852\n",
      "389 0.00010939054797681373\n",
      "390 0.00010431089933191156\n",
      "391 9.946753360612791e-05\n",
      "392 9.484995627415391e-05\n",
      "393 9.044802682146069e-05\n",
      "394 8.625184751382378e-05\n",
      "395 8.225042686009647e-05\n",
      "396 7.843529664288597e-05\n",
      "397 7.479758603540405e-05\n",
      "398 7.132917675380988e-05\n",
      "399 6.802228742168463e-05\n",
      "400 6.486927959705939e-05\n",
      "401 6.18632835751667e-05\n",
      "402 5.899719389747611e-05\n",
      "403 5.6264313022250503e-05\n",
      "404 5.3658307930174855e-05\n",
      "405 5.117325383546225e-05\n",
      "406 4.880387577852987e-05\n",
      "407 4.65446073378591e-05\n",
      "408 4.439018574074831e-05\n",
      "409 4.2335789789240166e-05\n",
      "410 4.037697689750471e-05\n",
      "411 3.850949897019531e-05\n",
      "412 3.672841637924294e-05\n",
      "413 3.502975771176331e-05\n",
      "414 3.340991692705392e-05\n",
      "415 3.186535288441129e-05\n",
      "416 3.039234361582685e-05\n",
      "417 2.8987762433483012e-05\n",
      "418 2.7648296997573165e-05\n",
      "419 2.6371161799497166e-05\n",
      "420 2.5153038694663387e-05\n",
      "421 2.3991269292056574e-05\n",
      "422 2.288335192360819e-05\n",
      "423 2.1826879912215048e-05\n",
      "424 2.0819202097782386e-05\n",
      "425 1.9858163998195844e-05\n",
      "426 1.894173239978736e-05\n",
      "427 1.806777296880678e-05\n",
      "428 1.723439207408587e-05\n",
      "429 1.6439355043242168e-05\n",
      "430 1.5681136594230123e-05\n",
      "431 1.495795665071368e-05\n",
      "432 1.426823258529038e-05\n",
      "433 1.3610469019469408e-05\n",
      "434 1.2983115659262931e-05\n",
      "435 1.2384712150767743e-05\n",
      "436 1.181411151262274e-05\n",
      "437 1.1269814817987944e-05\n",
      "438 1.0750633430867476e-05\n",
      "439 1.0255437508643422e-05\n",
      "440 9.783099366897005e-06\n",
      "441 9.332606788324956e-06\n",
      "442 8.902893269730883e-06\n",
      "443 8.49303163812286e-06\n",
      "444 8.102166806438945e-06\n",
      "445 7.729337131687557e-06\n",
      "446 7.373644593016921e-06\n",
      "447 7.034363978265648e-06\n",
      "448 6.710741934922313e-06\n",
      "449 6.402089615684217e-06\n",
      "450 6.107630473487231e-06\n",
      "451 5.8267473565949906e-06\n",
      "452 5.5588402115844615e-06\n",
      "453 5.303338245208118e-06\n",
      "454 5.059566490048574e-06\n",
      "455 4.827013174104581e-06\n",
      "456 4.60518565241523e-06\n",
      "457 4.393566984711749e-06\n",
      "458 4.1916997682903045e-06\n",
      "459 3.999152297616853e-06\n",
      "460 3.815458668959985e-06\n",
      "461 3.640249624357176e-06\n",
      "462 3.473099879419238e-06\n",
      "463 3.3136291310021873e-06\n",
      "464 3.1615115439382467e-06\n",
      "465 3.0163815402530125e-06\n",
      "466 2.87792988627647e-06\n",
      "467 2.745853269805287e-06\n",
      "468 2.619848207603381e-06\n",
      "469 2.499655664742199e-06\n",
      "470 2.3850097674479767e-06\n",
      "471 2.2756069574653684e-06\n",
      "472 2.1712325707451063e-06\n",
      "473 2.0716552619184533e-06\n",
      "474 1.9766673994548263e-06\n",
      "475 1.886039415919562e-06\n",
      "476 1.799569729805533e-06\n",
      "477 1.717077718946824e-06\n",
      "478 1.6383912285380478e-06\n",
      "479 1.5633239166962012e-06\n",
      "480 1.491685124786494e-06\n",
      "481 1.4233359045507728e-06\n",
      "482 1.3581297463367446e-06\n",
      "483 1.2959169038878593e-06\n",
      "484 1.2365619552111131e-06\n",
      "485 1.17992902394362e-06\n",
      "486 1.1258968471285087e-06\n",
      "487 1.074358764999114e-06\n",
      "488 1.0251739378273531e-06\n",
      "489 9.78244412336852e-07\n",
      "490 9.33468134176814e-07\n",
      "491 8.907435682537873e-07\n",
      "492 8.499796149949311e-07\n",
      "493 8.110879563317026e-07\n",
      "494 7.739789299051774e-07\n",
      "495 7.385762201605258e-07\n",
      "496 7.047945776382907e-07\n",
      "497 6.72558179471114e-07\n",
      "498 6.418020438487978e-07\n",
      "499 6.124515908945004e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99 412.40234375\n",
      "199 1.4771068096160889\n",
      "299 0.008386785164475441\n",
      "399 0.00019882740161847323\n",
      "499 3.4111573768313974e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99 439.16778564453125\n",
      "199 1.275431752204895\n",
      "299 0.0056249829940497875\n",
      "399 0.00014460907550528646\n",
      "499 2.6606776373228058e-05\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "# torch.backends.cuda.matmul.allow_tf32 = False  # Uncomment this to run on GPU\n",
    "\n",
    "# The above line disables TensorFloat32. This a feature that allows\n",
    "# networks to run at a much faster speed while sacrificing precision.\n",
    "# Although TensorFloat32 works well on most real models, for our toy model\n",
    "# in this tutorial, the sacrificed precision causes convergence issue.\n",
    "# For more information, see:\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ]
}