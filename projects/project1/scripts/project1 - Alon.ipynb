{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1602683126000,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "hE0xtnAn4pja"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(tx):\n",
    "    mean = np.mean(tx, axis=0)\n",
    "    std = np.std(tx, axis=0)\n",
    "    tx = (tx-mean)/std\n",
    "    return tx\n",
    "\n",
    "# function that add new features \n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=1 up to j=degree.\"\"\"\n",
    "    phi=np.zeros((x.shape[0],(degree+1)*x.shape[1]))\n",
    "    for j in range(degree+1):\n",
    "            phi[:,j*x.shape[1]:(j+1)*x.shape[1]]=x**j\n",
    "    return phi\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation_LR(y, x, k_fold, initial_w, max_iters, gamma, seed=1):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    loss_tr = [] \n",
    "    loss_te = []\n",
    "    ws = []\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in range(k_fold):\n",
    "        # ***************************************************\n",
    "        # get k'th subgroup in test, others in train\n",
    "        # ***************************************************\n",
    "        idx_tr = (np.delete(k_indices, k, 0)).flatten()\n",
    "        idx_te = k_indices[k]\n",
    "        x_tr, y_tr = x[idx_tr], y[idx_tr]\n",
    "        x_te, y_te = x[idx_te], y[idx_te]\n",
    "        # ***************************************************\n",
    "        # calculate the loss for train and test data\n",
    "        # ***************************************************\n",
    "        w, loss = logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "        loss_tr.append(loss)\n",
    "        loss_te.append(logistic_loss(y_te, x_te, w))\n",
    "        ws.append(w)\n",
    "    var_tr = np.var(loss_tr)\n",
    "    var_te = np.var(loss_te)\n",
    "    loss_tr = np.mean(loss_tr)\n",
    "    loss_te = np.mean(loss_te)\n",
    "    ws = np.mean(np.asarray(ws), axis=0)\n",
    "    return loss_tr, loss_te, var_tr, var_te, ws\n",
    "\n",
    "\n",
    "def cross_validation_RLR(y, x, k_fold, lambda_, initial_w, max_iters, gamma, seed=1):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    loss_tr = [] \n",
    "    loss_te = []\n",
    "    ws = []\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in range(k_fold):\n",
    "        # ***************************************************\n",
    "        # get k'th subgroup in test, others in train\n",
    "        # ***************************************************\n",
    "        idx_tr = (np.delete(k_indices, k, 0)).flatten()\n",
    "        idx_te = k_indices[k]\n",
    "        x_tr, y_tr = x[idx_tr], y[idx_tr]\n",
    "        x_te, y_te = x[idx_te], y[idx_te]\n",
    "        # ***************************************************\n",
    "        # calculate the loss for train and test data\n",
    "        # ***************************************************\n",
    "        w, loss = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)\n",
    "        loss_tr.append(loss)\n",
    "        loss_te.append(logistic_loss(y_te, x_te, w) + 0.5*lambda_*w.dot(w))\n",
    "        ws.append(w)\n",
    "    var_tr = np.var(loss_tr)\n",
    "    var_te = np.var(loss_te)\n",
    "    loss_tr = np.mean(loss_tr)\n",
    "    loss_te = np.mean(loss_te)\n",
    "    ws = np.mean(np.asarray(ws), axis=0)\n",
    "    return loss_tr, loss_te, var_tr, var_te, ws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First : exploring the data\n",
    "\n",
    "We'll need to have a look at what the data is, how it is distributed for the different features, and start to get an intuition about what methods might work better for analysis and prediction later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJIVugOC4pje"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "error",
     "timestamp": 1602683129008,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "6ywLVx4a4pje",
    "outputId": "6c3dc9cc-eaed-41d6-b186-c5c48d1c9066"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing methods work\n",
    "# initial_w = np.ones(tX.shape[1])\n",
    "# max_iters = 100\n",
    "# gamma = 0.1\n",
    "# lambda_ = 0.1\n",
    "# print('least_squares_GD')\n",
    "# print(least_squares_GD(y, tX, initial_w, max_iters, gamma))\n",
    "# print('least_squares_SGD')\n",
    "# print(least_squares_SGD(y, tX, initial_w, max_iters, gamma))\n",
    "# print('least_squares')\n",
    "# print(least_squares(y, tX))\n",
    "# print('ridge_regression')\n",
    "# print(ridge_regression(y, tX, lambda_))\n",
    "# print('logistic_regression SGD')\n",
    "# print(logistic_regression(y, tX, initial_w, max_iters, gamma, 'SGD'))\n",
    "# print('logistic_regression GD')\n",
    "# print(logistic_regression(y, tX, initial_w, max_iters, gamma, 'GD'))\n",
    "# print('reg_logistic_regression SGD')\n",
    "# print(reg_logistic_regression(y, tX, lambda_, initial_w, max_iters, gamma, 'SGD'))\n",
    "# print('reg_logistic_regression GD')\n",
    "# # print(reg_logistic_regression(y, tX, lambda_, initial_w, max_iters, gamma, 'GD'))\n",
    "# print('logistic_regression_newton')\n",
    "# print(logistic_regression_newton(y[:100], tX[:100], initial_w, max_iters, gamma))\n",
    "# print('reg_logistic_regression_newton')\n",
    "# print(reg_logistic_regression_newton(y[:100], tX[:100], lambda_, initial_w, max_iters, gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoPlUo5WU-CC"
   },
   "outputs": [],
   "source": [
    "# remove samples with error values\n",
    "idx_c = np.all(tX!=-999, axis=1)\n",
    "y_c = y[idx_c]\n",
    "tX_c = tX[idx_c]\n",
    "# regularize\n",
    "mean = np.mean(tX_c, axis=0)\n",
    "std = np.std(tX_c, axis=0)\n",
    "tX_c = (tX_c-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGLf-JCXU-CE"
   },
   "outputs": [],
   "source": [
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "print(\"NoErrors: s: \",np.sum(y_c==1),\", b: \",np.sum(y_c==-1),\" ,total:\",len(y_c))\n",
    "for n in range(tX_c.shape[1]):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(131)\n",
    "    plt.hist([tX_c[y_c==1,n],tX_c[y_c==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.legend(['s','b'])\n",
    "    plt.title('Feature '+str(n))\n",
    "    plt.subplot(132)\n",
    "    plt.title('s histogram feature '+str(n))\n",
    "    plt.hist(tX_c[y_c==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.subplot(133)\n",
    "    plt.title('b histogram feature '+str(n))\n",
    "    plt.hist(tX_c[y_c==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with error values\n",
    "idx_gf = np.arange(tX.shape[1])[np.all(tX!=-999, axis=0)]\n",
    "y_gf = y\n",
    "tX_gf = tX[:,idx_gf]\n",
    "# regularize\n",
    "mean = np.mean(tX_gf, axis=0)\n",
    "std = np.std(tX_gf, axis=0)\n",
    "tX_gf = (tX_gf-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "for n in range(tX_gf.shape[1]):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(131)\n",
    "    plt.hist([tX_gf[y_gf==1,n],tX_gf[y_gf==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.legend(['s','b'])\n",
    "    plt.title('Feature '+str(idx_gf[n]))\n",
    "    plt.subplot(132)\n",
    "    plt.title('s histogram feature '+str(idx_gf[n]))\n",
    "    plt.hist(tX_gf[y_gf==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.subplot(133)\n",
    "    plt.title('b histogram feature '+str(idx_gf[n]))\n",
    "    plt.hist(tX_gf[y_gf==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with error values\n",
    "y_jet = []\n",
    "tx_jet = []\n",
    "y_jet_nm = []\n",
    "tx_jet_nm = []\n",
    "# filtering according to undefinition due to jet number\n",
    "idx_jet_undef = [np.array([0,1,2,3,7,10,11,13,14,15,16,17,18,19,20,21,29]),\n",
    "                np.array([0,1,2,3,7,8,9,10,11,13,14,15,16,17,18,19,20,21,23,24,25,29]),\n",
    "                np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29]),\n",
    "                np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29])]\n",
    "# Extra filtering according to definition of mass\n",
    "idx_jet_undef_nm = [np.array([1,2,3,7,10,11,13,14,15,16,17,18,19,20,21,29]),\n",
    "                    np.array([1,2,3,7,8,9,10,11,13,14,15,16,17,18,19,20,21,23,24,25,29]),\n",
    "                    np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29]),\n",
    "                    np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29])]\n",
    "for jet in range(4):\n",
    "    idx_jet = tX[:,22]==jet\n",
    "    y_jet.append(y[idx_jet])\n",
    "    tx_jet.append(tX[idx_jet][:,idx_jet_undef[jet]])\n",
    "    # tx_jet.append(standardize(tX[idx_jet]))\n",
    "for jet in range(4):\n",
    "    idx_jet = tX[:,22]==jet\n",
    "    y_jet_nm.append(y[idx_jet])\n",
    "    tx_jet_nm.append(tX[idx_jet][:,idx_jet_undef_nm[jet]])\n",
    "    # tx_jet.append(standardize(tX[idx_jet]))\n",
    "\n",
    "for jet in range(4):\n",
    "    print('Jet {:} shape is {:}'.format(jet,tx_jet[jet].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "for jet in range(4):\n",
    "    print('Jet {:}: s: {:}, b: {:} ,total: {:}'.format(jet, np.sum(y_jet[jet]==1),np.sum(y_jet[jet]==-1),len(y_jet[jet])))\n",
    "    for n,feat in enumerate(idx_jet_undef[jet]):\n",
    "        plt.figure(figsize=(20,4))\n",
    "        plt.subplot(131)\n",
    "        plt.hist([tx_jet[jet][y_jet[jet]==1,n],tx_jet[jet][y_jet[jet]==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "        plt.legend(['s','b'])\n",
    "        plt.title('Feature '+str(feat))\n",
    "        plt.subplot(132)\n",
    "        plt.title('s histogram feature '+str(feat))\n",
    "        plt.hist(tx_jet[jet][y_jet[jet]==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "        plt.subplot(133)\n",
    "        plt.title('b histogram feature '+str(feat))\n",
    "        plt.hist(tx_jet[jet][y_jet[jet]==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "for jet in range(4):\n",
    "    print('Jet {:}: s: {:}, b: {:} ,total: {:}'.format(jet, np.sum(y_jet_nm[jet]==1),np.sum(y_jet_nm[jet]==-1),len(y_jet_nm[jet])))\n",
    "    for n,feat in enumerate(idx_jet_undef_nm[jet]):\n",
    "        plt.figure(figsize=(20,4))\n",
    "        plt.subplot(131)\n",
    "        plt.hist([tx_jet_nm[jet][y_jet_nm[jet]==1,n],tx_jet_nm[jet][y_jet_nm[jet]==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "        plt.legend(['s','b'])\n",
    "        plt.title('Feature '+str(feat))\n",
    "        plt.subplot(132)\n",
    "        plt.title('s histogram feature '+str(feat))\n",
    "        plt.hist(tx_jet_nm[jet][y_jet_nm[jet]==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "        plt.subplot(133)\n",
    "        plt.title('b histogram feature '+str(feat))\n",
    "        plt.hist(tx_jet_nm[jet][y_jet_nm[jet]==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5iFp9zV4pjg"
   },
   "source": [
    "# Actual predictions start from here\n",
    "\n",
    "After having looked at the data we will now do some actual predictions using different models andd parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_full = tx_jet+tx_jet_nm\n",
    "y_full = y_jet+y_jet_nm\n",
    "# for y_i in y_full:\n",
    "#     y_i[y_i==-1] = 0\n",
    "max_iters = 1000\n",
    "k_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "gammas = np.geomspace(0.001,2,15)\n",
    "loss_tr = np.zeros((8,len(gammas)))\n",
    "loss_te = np.zeros((8,len(gammas)))\n",
    "var_tr = np.zeros((8,len(gammas)))\n",
    "var_te = np.zeros((8,len(gammas)))\n",
    "for i in range(8):\n",
    "    tx_i = tx_full[i]\n",
    "    y_i = y_full[i]\n",
    "    for g,gamma in enumerate(gammas):\n",
    "        initial_w = np.zeros(tx_i.shape[1])\n",
    "        loss_tr[i,g],loss_te[i,g], var_tr[i,g], var_te[i,g], ws = cross_validation_LR(y_i, tx_i, k_fold, initial_w, max_iters, gamma)\n",
    "        print('set {:} - train_loss: {:}, test_loss: {:}, train_var: {:}, test_var: {:}'.format(i,loss_tr[i,g],loss_te[i,g], var_tr[i,g], var_te[i,g]))\n",
    "        print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regularized logistic regression\n",
    "# gammas = np.geomspace(1e-8,1e-5,8)\n",
    "gamma = 1e-9\n",
    "lambdas = np.geomspace(1e-4,1,5)\n",
    "seeds = np.arange(100)\n",
    "# loss_tr = np.zeros((8,len(lambdas),len(gammas)))\n",
    "# loss_te = np.zeros((8,len(lambdas),len(gammas)))\n",
    "# var_tr = np.zeros((8,len(lambdas),len(gammas)))\n",
    "# var_te = np.zeros((8,len(lambdas),len(gammas)))\n",
    "loss_tr = np.zeros((8,len(seeds),len(lambdas)))\n",
    "loss_te = np.zeros((8,len(seeds),len(lambdas)))\n",
    "var_tr = np.zeros((8,len(seeds),len(lambdas)))\n",
    "var_te = np.zeros((8,len(seeds),len(lambdas)))\n",
    "for i in range(8):\n",
    "    tx_i = tx_full[i]\n",
    "    y_i = y_full[i]\n",
    "    for l,lambda_ in enumerate(lambdas):\n",
    "#         for g,gamma in enumerate(gammas):\n",
    "        for s,seed in enumerate(seeds):\n",
    "            initial_w = np.zeros(tx_i.shape[1])\n",
    "    #         loss_tr[i,l,g],loss_te[i,l,g], var_tr[i,l,g], var_te[i,l,g], ws = cross_validation_RLR(y_i, tx_i, k_fold, lambda_, initial_w, max_iters, gamma)\n",
    "    #         print('set {:} lambda {:} gamma {:} -\\n train_loss: {:.6f}, test_loss: {:.6f}, train_var: {:.6f}, test_var: {:.6f}'.format(i,lambda_,gamma,loss_tr[i,l,g],loss_te[i,l,g], var_tr[i,l,g], var_te[i,l,g]))\n",
    "            loss_tr[i,s,l],loss_te[i,s,l], var_tr[i,s,l], var_te[i,s,l], ws = cross_validation_RLR(y_i, tx_i, k_fold, lambda_, initial_w, max_iters, gamma, seed)\n",
    "            print('set {:} lambda {:}-\\n train_loss: {:.6f}, test_loss: {:.6f}, train_var: {:.6f}, test_var: {:.6f}'.format(i,lambda_,loss_tr[i,s,l],loss_te[i,s,l], var_tr[i,s,l], var_te[i,s,l]))\n",
    "    #         print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print('Average Loss for set {} is: train: {}, test: {}'.format(i,np.mean(loss_tr[i]),np.mean(loss_te[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Loss for set 0 is: train: 54980.74763044169, test: 13745.20416065596<br>\n",
    "Average Loss for set 1 is: train: 40983.29810703878, test: 10245.85299564541<br>\n",
    "Average Loss for set 2 is: train: 21006.93861075539, test: 5251.771478499431<br>\n",
    "Average Loss for set 3 is: train: 10771.50148604793, test: 2692.925730373346<br>\n",
    "Average Loss for set 4 is: train: 55011.27157849578, test: 13752.82139825698<br>\n",
    "Average Loss for set 5 is: train: 41129.31680385559, test: 10282.34753957150<br>\n",
    "Average Loss for set 6 is: train: 21107.61567660550, test: 5276.935237919042<br>\n",
    "Average Loss for set 7 is: train: 10792.49529019947, test: 2698.171831750048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,var in enumerate(var_te):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    for seed in enumerate(seeds):\n",
    "        plt.plot(loss_te[i,seed], 'b')\n",
    "    plt.\n",
    "    plt.title('Loss of Features set '+str(i))\n",
    "    plt.xlabel('lambda')\n",
    "    plt.ylabel('error')\n",
    "    plt.subplot(122)\n",
    "    for seed in range(len(seeds)):\n",
    "        plt.plot(var[seed])\n",
    "        plt.title('Variation of Loss of Features set '+str(i))\n",
    "        plt.xlabel('lambda')\n",
    "        plt.ylabel('variation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,var in enumerate(var_te):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolor(loss_te[i])\n",
    "    plt.colorbar()\n",
    "    plt.title('Loss of Features set '+str(i))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('lambda')\n",
    "    plt.subplot(122)\n",
    "    plt.pcolor(np.log10(var))\n",
    "    plt.colorbar()\n",
    "    plt.title('Variation of Features set '+str(i))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('lambda')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TEST_PATH = '../data/test.csv.zip' \n",
    "# test_y, test_X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql2-oUuA4ynz"
   },
   "source": [
    "## Generate predictions using only features with no errrors throughought\n",
    "\n",
    "This enables us to use some of the methods from the course directly, without having to adjust some of the functionnality to account for the fact that a lot of errors are in the dataset. First let us see which features from the test dataset are error free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "test_x_df = pd.read_csv(DATA_TEST_PATH, index_col='Id').drop('Prediction', axis='columns')\n",
    "test_x_df\n",
    "\n",
    "train_x_df = pd.read_csv(DATA_TRAIN_PATH, index_col='Id').drop('Prediction', axis='columns')\n",
    "train_y_df = pd.read_csv(DATA_TRAIN_PATH, index_col='Id', usecols=['Id', 'Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with -999 errors\n",
    "error_cols_te = np.full(test_x_df.shape[1], False)\n",
    "for i in range(test_x_df.shape[1]):\n",
    "#     print(test_x_df.iloc[:,i].values)\n",
    "#     print(-999.0 in test_x_df.iloc[:,i].values)\n",
    "    if -999.0 in test_x_df.iloc[:,i].values:\n",
    "        error_cols_te[i] = True\n",
    "print(f\"There are {error_cols_te.tolist().count(True)} test error columns are which are : \\n{error_cols_te}\\n\")\n",
    "\n",
    "error_cols_tr = np.full(train_x_df.shape[1], False)\n",
    "for i in range(train_x_df.shape[1]):\n",
    "    if -999.0 in train_x_df.iloc[:,i].values:\n",
    "        error_cols_tr[i] = True\n",
    "print(f\"There are {error_cols_tr.tolist().count(True)} train error columns are which are : \\n{error_cols_tr}\")\n",
    "\n",
    "no_error_cols_tr_te = (~(error_cols_tr | error_cols_te)).tolist()\n",
    "print(f\"They have {no_error_cols_tr_te.count(True)} following shared non error columns :\\n{no_error_cols_tr_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this figured out we can now extract the valid columns from test and train data, do some training and testing on data, then generate answers for the test data and submit to aicrowd !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkMBGpay4pjh"
   },
   "source": [
    "# Save prediction ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i--OcRst4pjh"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grQDhIJ84pjk"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
