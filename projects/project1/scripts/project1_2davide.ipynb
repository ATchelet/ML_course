{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1602683126000,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "hE0xtnAn4pja"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualitation import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(tx):\n",
    "    mean = np.mean(tx, axis=0)\n",
    "    std = np.std(tx, axis=0)\n",
    "    tx = (tx-mean)/std\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First : exploring the data\n",
    "\n",
    "We'll need to have a look at what the data is, how it is distributed for the different features, and start to get an intuition about what methods might work better for analysis and prediction later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJIVugOC4pje"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "error",
     "timestamp": 1602683129008,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "6ywLVx4a4pje",
    "outputId": "6c3dc9cc-eaed-41d6-b186-c5c48d1c9066"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "# loading train data\n",
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoPlUo5WU-CC"
   },
   "outputs": [],
   "source": [
    "# remove samples with error values\n",
    "idx_c = np.all(tX!=-999, axis=1)\n",
    "y_c = y[idx_c]\n",
    "tX_c = tX[idx_c]\n",
    "\n",
    "# regularize\n",
    "mean = np.mean(tX_c, axis=0)\n",
    "std = np.std(tX_c, axis=0)\n",
    "tX_c = (tX_c-mean)/std # tX.c contains data without samples with errors\n",
    "\n",
    "# checking if features have errors or not\n",
    "f=np.all(tX!=-999, axis=0)\n",
    "\n",
    "# Removing features with errors (-999)\n",
    "tX_fc = tX_c[:,f]\n",
    "\n",
    "# adding column of 1 at the beginning of tX_fc\n",
    "tX_fc= np.hstack(( np.ones((tX_fc.shape[0], 1), dtype=tX_fc.dtype),tX_fc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing features: True-> No error False -> At least one error\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f[22],any(tX[:,22]>3)==True) \n",
    "# This shows that the features number of jets has no errors and only four possible values: 0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGLf-JCXU-CE"
   },
   "outputs": [],
   "source": [
    "#plot of all features\n",
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "print(\"NoErrors: s: \",np.sum(y_c==1),\", b: \",np.sum(y_c==-1),\" ,total:\",len(y_c))\n",
    "for n in range(tX_c.shape[1]):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(131)\n",
    "    plt.hist([tX_c[y_c==1,n],tX_c[y_c==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.legend(['s','b'])\n",
    "    plt.title('Feature '+str(n))\n",
    "    plt.subplot(132)\n",
    "    plt.title('s histogram feature '+str(n))\n",
    "    plt.hist(tX_c[y_c==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.subplot(133)\n",
    "    plt.title('b histogram feature '+str(n))\n",
    "    plt.hist(tX_c[y_c==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features with error values\n",
    "idx_gf = np.arange(tX.shape[1])[np.all(tX!=-999, axis=0)]\n",
    "y_gf = y\n",
    "tX_gf = tX[:,idx_gf]\n",
    "# regularize\n",
    "mean = np.mean(tX_gf, axis=0)\n",
    "std = np.std(tX_gf, axis=0)\n",
    "tX_gf = (tX_gf-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of features without errors\n",
    "print(\"Overall: s: \",np.sum(y==1),\", b: \",np.sum(y==-1),\" ,total:\",len(y))\n",
    "for n in range(tX_gf.shape[1]):\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.subplot(131)\n",
    "    plt.hist([tX_gf[y_gf==1,n],tX_gf[y_gf==-1,n]], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.legend(['s','b'])\n",
    "    plt.title('Feature '+str(idx_gf[n]))\n",
    "    plt.subplot(132)\n",
    "    plt.title('s histogram feature '+str(idx_gf[n]))\n",
    "    plt.hist(tX_gf[y_gf==1,n], 20, density=True, histtype='bar', stacked=True)\n",
    "    plt.subplot(133)\n",
    "    plt.title('b histogram feature '+str(idx_gf[n]))\n",
    "    plt.hist(tX_gf[y_gf==-1,n], 20, density=True, histtype='bar', stacked=True)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5iFp9zV4pjg"
   },
   "source": [
    "# Actual predictions start from here\n",
    "\n",
    "After having looked at the data we will now do some actual predictions using different models andd parameters. \n",
    "Feature 15 ,18, 20 does not seem to impact the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting samples according to number of jets (featuare 23) \n",
    "# Only four possible values for jets (3 indicates also more jets)\n",
    "jet_0 = tX[:,22]==0\n",
    "jet_1 = tX[:,22]==1\n",
    "jet_2 = tX[:,22]==2\n",
    "jet_3 = tX[:,22]==3\n",
    "\n",
    "#Splitting tX and y\n",
    "tX_0 = tX[jet_0, :]\n",
    "tX_1 = tX[jet_1, :]\n",
    "tX_2 = tX[jet_2, :]\n",
    "tX_3 = tX[jet_3, :]\n",
    "y_0 = y[jet_0]\n",
    "y_1 = y[jet_1]\n",
    "y_2 = y[jet_2]\n",
    "y_3 = y[jet_3]\n",
    "\n",
    "#Removing column with jet number\n",
    "tX_0=np.delete(tX_0,22,1)\n",
    "tX_1=np.delete(tX_1,22,1)\n",
    "tX_2=np.delete(tX_2,22,1)\n",
    "tX_3=np.delete(tX_3,22,1)\n",
    "\n",
    "# selection of features without errors in every partition of tX\n",
    "f_0 =np.all(tX_0!=-999, axis=0)\n",
    "tX_0_gf = tX_0[:,f_0]\n",
    "f_1 =np.all(tX_1!=-999, axis=0)\n",
    "tX_1_gf = tX_1[:,f_1]\n",
    "f_2 =np.all(tX_2!=-999, axis=0)\n",
    "tX_2_gf = tX_2[:,f_2]\n",
    "f_3 =np.all(tX_3!=-999, axis=0)\n",
    "tX_3_gf = tX_3[:,f_3]\n",
    "tX_3_gf.shape[0] + tX_2_gf.shape[0] + tX_1_gf.shape[0] + tX_0_gf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing last column of tX_0_gf \n",
    "tX_0_gf = tX_0_gf[:,0:tX_0_gf.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "tX_0_gf = standardize(tX_0_gf)\n",
    "tX_1_gf = standardize(tX_1_gf)\n",
    "tX_2_gf = standardize(tX_2_gf)\n",
    "tX_3_gf = standardize(tX_3_gf)\n",
    "tX_0_gf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that add new features \n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=1 up to j=degree.\"\"\"\n",
    "    phi=np.zeros((x.shape[0],degree*x.shape[1]))\n",
    "    for j in range(degree):\n",
    "            phi[:,j*x.shape[1]:(j+1)*x.shape[1]]=x**(j+1)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing to polynomial regression of degree= 'degree'\n",
    "degree = 7\n",
    "tX_0_gf=build_poly(tX_0_gf, degree)\n",
    "tX_1_gf=build_poly(tX_1_gf, degree)\n",
    "tX_2_gf=build_poly(tX_2_gf, degree)\n",
    "tX_3_gf=build_poly(tX_3_gf, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding column of 1 at the beginning of tX_gf\n",
    "tX_0_gf= np.hstack(( np.ones((tX_0_gf.shape[0], 1), dtype=tX_0_gf.dtype),tX_0_gf))\n",
    "tX_1_gf= np.hstack(( np.ones((tX_1_gf.shape[0], 1), dtype=tX_1_gf.dtype),tX_1_gf))\n",
    "tX_2_gf= np.hstack(( np.ones((tX_2_gf.shape[0], 1), dtype=tX_2_gf.dtype),tX_2_gf))\n",
    "tX_3_gf= np.hstack(( np.ones((tX_3_gf.shape[0], 1), dtype=tX_3_gf.dtype),tX_3_gf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations_2 import *\n",
    "\n",
    "# indices of samples with no errors\n",
    "ids_=ids[idx_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least_squares method \n",
    "w_ls_0, loss_ls_0 =least_squares(y_0,tX_0_gf)\n",
    "w_ls_1, loss_ls_1 =least_squares(y_1,tX_1_gf)\n",
    "w_ls_2, loss_ls_2 =least_squares(y_2,tX_2_gf)\n",
    "w_ls_3, loss_ls_3 =least_squares(y_3,tX_3_gf)\n",
    "loss_ls_0, loss_ls_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least square using SGD\n",
    "\n",
    "# starting value for SGD method\n",
    "w_in_0= np.zeros((tX_0_gf.shape[1],))\n",
    "w_in_1= np.zeros((tX_1_gf.shape[1],))\n",
    "w_in_2= np.zeros((tX_2_gf.shape[1],))\n",
    "w_in_3= np.zeros((tX_3_gf.shape[1],))\n",
    "\n",
    "# number of iterations for SGD method\n",
    "max_iters=1000\n",
    "\n",
    "# step-size of the method\n",
    "gamma=0.01\n",
    "\n",
    "# application of the method to the separated dataset\n",
    "w_ls_SGD_0,_=least_squares_SGD(y_0,tX_0_gf, w_in_0, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_SGD_1,_=least_squares_SGD(y_1,tX_1_gf, w_in_1, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_SGD_2,_=least_squares_SGD(y_2,tX_2_gf, w_in_2, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_SGD_3,_=least_squares_SGD(y_3,tX_3_gf, w_in_3, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### logistic regression SGD############\n",
    "\n",
    "# modifing y from -1,1 to 0,1 \n",
    "y_l=(y_gf+1)/2\n",
    "\n",
    "# starting value for GD/SGD method\n",
    "w_initial= np.zeros((19,))\n",
    "\n",
    "# number of iterations for SGD/GD method\n",
    "max_iters=100\n",
    "\n",
    "# step pf the method\n",
    "gamma=0.00001\n",
    "\n",
    "_, w_l = logistic_regression(y_l,tX_gf, w_initial,max_iters,gamma, mode= 'GD')\n",
    "w_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized logistic regression\n",
    "\n",
    "# starting value for GD/SGD method\n",
    "w_initial_rl = w_ls\n",
    "\n",
    "# number of iterations for SGD/GD method\n",
    "max_iters=100\n",
    "\n",
    "# step pf the method\n",
    "gamma=0.1\n",
    "lambda_=1\n",
    "reg_logistic_regression(y_l, tX_gf, lambda_, w_initial_rl, max_iters, gamma, mode='SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkMBGpay4pjh"
   },
   "source": [
    "# Save prediction ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i--OcRst4pjh"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting samples according to number of jets (featuare 23) \n",
    "# Only four possible values for jets (3 indicates also more jets)\n",
    "jet_0_test = tX_test[:,22]==0\n",
    "jet_1_test = tX_test[:,22]==1\n",
    "jet_2_test = tX_test[:,22]==2\n",
    "jet_3_test = tX_test[:,22]==3\n",
    "\n",
    "#Splitting tX \n",
    "tX_0_test = tX_test[jet_0_test, :]\n",
    "tX_1_test = tX_test[jet_1_test, :]\n",
    "tX_2_test = tX_test[jet_2_test, :]\n",
    "tX_3_test = tX_test[jet_3_test, :]\n",
    "\n",
    "# Splitting indices\n",
    "ids_test_0 = ids_test[jet_0_test]\n",
    "ids_test_1 = ids_test[jet_1_test]\n",
    "ids_test_2 = ids_test[jet_2_test]\n",
    "ids_test_3 = ids_test[jet_3_test]\n",
    "\n",
    "\n",
    "#Removing column with jet number\n",
    "tX_0_test=np.delete(tX_0_test,22,1)\n",
    "tX_1_test=np.delete(tX_1_test,22,1)\n",
    "tX_2_test=np.delete(tX_2_test,22,1)\n",
    "tX_3_test=np.delete(tX_3_test,22,1)\n",
    "\n",
    "# selection of features without errors in every partition of tX\n",
    "f_0_test =np.all(tX_0_test!=-999, axis=0)\n",
    "tX_0_gf_test = tX_0_test[:,f_0_test]\n",
    "f_1_test =np.all(tX_1_test!=-999, axis=0)\n",
    "tX_1_gf_test = tX_1_test[:,f_1_test]\n",
    "f_2_test =np.all(tX_2_test!=-999, axis=0)\n",
    "tX_2_gf_test = tX_2_test[:,f_2_test]\n",
    "f_3_test =np.all(tX_3_test!=-999, axis=0)\n",
    "tX_3_gf_test = tX_3_test[:,f_3_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing last column of tX_0_gf \n",
    "tX_0_gf_test = tX_0_gf_test[:,0:tX_0_gf_test.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "tX_0_gf_test = standardize(tX_0_gf_test)\n",
    "tX_1_gf_test = standardize(tX_1_gf_test)\n",
    "tX_2_gf_test = standardize(tX_2_gf_test)\n",
    "tX_3_gf_test = standardize(tX_3_gf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0_gf_test=build_poly(tX_0_gf_test, degree)\n",
    "tX_1_gf_test=build_poly(tX_1_gf_test, degree)\n",
    "tX_2_gf_test=build_poly(tX_2_gf_test, degree)\n",
    "tX_3_gf_test=build_poly(tX_3_gf_test, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 column \n",
    "tX_0_gf_test= np.hstack(( np.ones((tX_0_gf_test.shape[0], 1), dtype=tX_0_gf_test.dtype),tX_0_gf_test))\n",
    "tX_1_gf_test= np.hstack(( np.ones((tX_1_gf_test.shape[0], 1), dtype=tX_1_gf_test.dtype),tX_1_gf_test))\n",
    "tX_2_gf_test= np.hstack(( np.ones((tX_2_gf_test.shape[0], 1), dtype=tX_2_gf_test.dtype),tX_2_gf_test))\n",
    "tX_3_gf_test= np.hstack(( np.ones((tX_3_gf_test.shape[0], 1), dtype=tX_3_gf_test.dtype),tX_3_gf_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using least squares\n",
    "y_pred_0 = predict_labels(w_ls_0,tX_0_gf_test)\n",
    "y_pred_1 = predict_labels(w_ls_1,tX_1_gf_test)\n",
    "y_pred_2 = predict_labels(w_ls_2,tX_2_gf_test)\n",
    "y_pred_3 = predict_labels(w_ls_3,tX_3_gf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions together\n",
    "y_pred=np.zeros((ids_test.shape))\n",
    "y_pred[jet_0_test]=y_pred_0\n",
    "y_pred[jet_1_test]=y_pred_1\n",
    "y_pred[jet_2_test]=y_pred_2\n",
    "y_pred[jet_3_test]=y_pred_3\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoPlUo5WU-CC"
   },
   "outputs": [],
   "source": [
    "# remove features with errors from test data\n",
    "f_test= np.all(tX_test!=-999, axis=0)\n",
    "\n",
    "tX_test_fc = tX_test[:,f_test]\n",
    "# regularize\n",
    "mean = np.mean(tX_test_fc, axis=0)\n",
    "std = np.std(tX_test_fc, axis=0)\n",
    "tX_test_fc = (tX_test_fc-mean)/std\n",
    "\n",
    "#Adding 1 column\n",
    "tX_test_fc= np.hstack(( np.ones((tX_test_fc.shape[0], 1), dtype=tX_fc.dtype),tX_test_fc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we use logistic regression SGD and no error samples in test data\n",
    "#weights=w_ls_GD\n",
    "#y_pred = prediction(tX_test_fc, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grQDhIJ84pjk"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'sub_3' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test_fc)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n",
    "# hello###############################Ã "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
