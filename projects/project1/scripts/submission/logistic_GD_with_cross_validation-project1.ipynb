{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1602683126000,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "hE0xtnAn4pja"
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid (logistic) function\n",
    "def logistic_function(x,w):\n",
    "    l=np.exp(x.dot(w))/(1+np.exp(x.dot(w)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with logistic regression\n",
    "def logistic_prediction(w,x):\n",
    "    y_pred_l=np.zeros((x.shape[0],))\n",
    "    y_pred_l[np.where(logistic_function(x,w)<0.5)] = -1\n",
    "    y_pred_l[np.where(logistic_function(x,w)>=0.5)] = 1\n",
    "    return y_pred_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(tx):\n",
    "    mean = np.mean(tx, axis=0)\n",
    "    std = np.std(tx, axis=0)\n",
    "    tx = (tx-mean)/std\n",
    "    return tx\n",
    "\n",
    "# function that add new features with polynomial expansion it also adds offset (all 1 column)\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    phi=np.zeros((x.shape[0],degree*x.shape[1]))\n",
    "    for j in range(degree):\n",
    "            phi[:,j*x.shape[1]:(j+1)*x.shape[1]]=x**(j+1)\n",
    "    phi = np.hstack(( np.ones((phi.shape[0], 1), dtype=phi.dtype),phi))\n",
    "    return phi \n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "#Cross validation for logistic regression\n",
    "def cross_validation_LR(y, x, k_fold, initial_w, max_iters, gamma, seed=1):\n",
    "    loss_tr = [] \n",
    "    loss_te = []\n",
    "    ws = []\n",
    "    acc=[]\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in range(k_fold):\n",
    "        # ***************************************************\n",
    "        # get k'th subgroup in test, others in train\n",
    "        # ***************************************************\n",
    "        idx_tr = (np.delete(k_indices, k, 0)).flatten()\n",
    "        idx_te = k_indices[k]\n",
    "        x_tr, y_tr = x[idx_tr], y[idx_tr]\n",
    "        x_te, y_te = x[idx_te], y[idx_te]\n",
    "        mean = np.mean(x_tr, axis=0)\n",
    "        std = np.std(x_tr, axis=0)\n",
    "        for j in range (x_tr.shape[1]):\n",
    "            #Because assuming there is an all one column vector at the beginning\n",
    "            if j>0:\n",
    "                x_tr[:,j]=(x_tr[:,j]-mean[j])/std[j]\n",
    "                #Standardization of the test with the mean and std of the training \n",
    "                x_te[:,j]=(x_te[:,j]-mean[j])/std[j]\n",
    "        # ***************************************************\n",
    "        # calculate the loss for train and test data\n",
    "        # ***************************************************\n",
    "        w, loss = logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma, mode='GD')\n",
    "        y_out_test = logistic_prediction(w,x_te)\n",
    "        accuracy=100*(y_out_test==(y_te*2-1)).tolist().count(True)/y_out_test.shape[0]\n",
    "        loss_tr.append(loss)\n",
    "        loss_te.append(logistic_loss(y_te, x_te, w))\n",
    "        ws.append(w)\n",
    "        acc.append(accuracy)\n",
    "    var_tr = np.var(loss_tr)\n",
    "    var_te = np.var(loss_te)\n",
    "    loss_tr = np.mean(loss_tr)\n",
    "    loss_te = np.mean(loss_te)\n",
    "    ws = np.mean(np.asarray(ws), axis=0)\n",
    "    acc=np.mean(np.asarray(acc), axis=0)\n",
    "    return loss_tr, loss_te, var_tr, var_te, ws, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First : exploring the data\n",
    "\n",
    "We'll need to have a look at what the data is, how it is distributed for the different features, and start to get an intuition about what methods might work better for analysis and prediction later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJIVugOC4pje"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "error",
     "timestamp": 1602683129008,
     "user": {
      "displayName": "Davide Rosso",
      "photoUrl": "",
      "userId": "18358579221259765961"
     },
     "user_tz": -120
    },
    "id": "6ywLVx4a4pje",
    "outputId": "6c3dc9cc-eaed-41d6-b186-c5c48d1c9066"
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 0 shape is (99913, 16)\n",
      "Jet 1 shape is (77544, 22)\n",
      "Jet 2 shape is (50379, 29)\n",
      "Jet 3 shape is (22164, 29)\n"
     ]
    }
   ],
   "source": [
    "# remove features with error values\n",
    "y_jet = []\n",
    "tx_jet = []\n",
    "y_jet_nm = []\n",
    "tx_jet_nm = []\n",
    "# filtering according to undefinition due to jet number\n",
    "idx_jet_undef = [np.array([0,1,2,3,7,10,11,13,14,15,16,17,18,19,20,21]),\n",
    "                np.array([0,1,2,3,7,8,9,10,11,13,14,15,16,17,18,19,20,21,23,24,25,29]),\n",
    "                np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29]),\n",
    "                np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29])]\n",
    "# Extra filtering according to definition of mass\n",
    "idx_jet_undef_nm = [np.array([1,2,3,7,10,11,13,14,15,16,17,18,19,20,21]),\n",
    "                    np.array([1,2,3,7,8,9,10,11,13,14,15,16,17,18,19,20,21,23,24,25,29]),\n",
    "                    np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29]),\n",
    "                    np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,23,24,25,26,27,28,29])]\n",
    "for jet in range(4):\n",
    "    idx_jet = tX[:,22]==jet\n",
    "    y_jet.append(y[idx_jet])\n",
    "    tx_jet.append(tX[idx_jet][:,idx_jet_undef[jet]])\n",
    "    #tx_jet.append(standardize(tX[idx_jet][:,idx_jet_undef[jet]]))\n",
    "for jet in range(4):\n",
    "    idx_jet = tX[:,22]==jet\n",
    "    y_jet_nm.append(y[idx_jet])\n",
    "    tx_jet_nm.append(tX[idx_jet][:,idx_jet_undef_nm[jet]])\n",
    "    #tx_jet.append(standardize(tX[idx_jet][:,idx_jet_undef_nm[jet]]))\n",
    "\n",
    "for jet in range(4):\n",
    "    print('Jet {:} shape is {:}'.format(jet,tx_jet[jet].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5iFp9zV4pjg"
   },
   "source": [
    "# Actual predictions start from here\n",
    "\n",
    "After having looked at the data we will now do some actual predictions using different models andd parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_full = tx_jet+tx_jet_nm\n",
    "y_full = y_jet+y_jet_nm\n",
    "# for y_i in y_full:\n",
    "#     y_i[y_i==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set 0 - accuracy:82.60269036752342\n",
      "set 1 - accuracy:71.23310636541835\n",
      "set 2 - accuracy:73.98364300460538\n",
      "set 3 - accuracy:73.09149972929076\n",
      "set 4 - accuracy:82.5876771558972\n",
      "set 5 - accuracy:71.2021561951924\n",
      "set 6 - accuracy:73.91019533111006\n",
      "set 7 - accuracy:72.9741923840462\n"
     ]
    }
   ],
   "source": [
    "# logistic regression cross validation \n",
    "k_fold=6\n",
    "degree = 1 \n",
    "max_iters_LR=4000\n",
    "#loss_tr = np.zeros((8,len(degrees)))\n",
    "#loss_te = np.zeros((8,len(degrees)))\n",
    "#var_tr = np.zeros((8,len(degrees)))\n",
    "#var_te = np.zeros((8,len(degrees)))\n",
    "acc = np.zeros((8))\n",
    "for i in range(8):\n",
    "    tx_i = tx_full[i]\n",
    "    y_i = (y_full[i]+1)/2 # Trasforming y in an array of 0/1 instead of -1/1\n",
    "    gamma=(1e-1)/(len(y_i)*(k_fold-1)/k_fold)\n",
    "    seed = 1\n",
    "    initial_w = np.zeros(build_poly(tx_i,degree).shape[1],)\n",
    "    _, _, _, _, ws, acc[i] = cross_validation_LR(y_i, build_poly(tx_i,degree), k_fold, initial_w, max_iters_LR, gamma, seed)\n",
    "    #print('set {:} - train_loss: {:}, test_loss: {:}, train_var: {:}, test_var: {:} accuracy:{:}'.format(i,loss_tr[i,g],loss_te[i,g], var_tr[i,g], var_te[i,g],acc[i]))\n",
    "        #print(ws)\n",
    "    print('set {:} - accuracy:{:}'.format(i,acc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set=0 Current iteration=0,set=0, loss=0.6763342208300869\n",
      "set=0 Current iteration=100,set=0, loss=0.44181866378390905\n",
      "set=0 Current iteration=200,set=0, loss=0.4266163193775543\n",
      "set=0 Current iteration=300,set=0, loss=0.42045239192561606\n",
      "set=0 Current iteration=400,set=0, loss=0.41627032198006464\n",
      "set=0 Current iteration=500,set=0, loss=0.413002880724155\n",
      "set=0 Current iteration=600,set=0, loss=0.41032980288810084\n",
      "set=0 Current iteration=700,set=0, loss=0.40809406858891834\n",
      "set=0 Current iteration=800,set=0, loss=0.4061969316287591\n",
      "set=0 Current iteration=900,set=0, loss=0.4045694993759145\n",
      "set=0 Current iteration=1000,set=0, loss=0.4031612584273638\n",
      "set=0 Current iteration=1100,set=0, loss=0.4019339936782408\n",
      "set=0 Current iteration=1200,set=0, loss=0.4008580992495643\n",
      "set=0 Current iteration=1300,set=0, loss=0.3999101868913331\n",
      "set=0 Current iteration=1400,set=0, loss=0.3990714714640561\n",
      "set=0 Current iteration=1500,set=0, loss=0.39832664775185944\n",
      "set=0 Current iteration=1600,set=0, loss=0.39766308988536003\n",
      "set=0 Current iteration=1700,set=0, loss=0.3970702689983206\n",
      "set=0 Current iteration=1800,set=0, loss=0.3965393220993542\n",
      "set=0 Current iteration=1900,set=0, loss=0.39606272779247786\n",
      "set=0 Current iteration=2000,set=0, loss=0.39563405878422353\n",
      "set=0 Current iteration=2100,set=0, loss=0.395247790471482\n",
      "set=0 Current iteration=2200,set=0, loss=0.39489915116241237\n",
      "set=0 Current iteration=2300,set=0, loss=0.3945840036251688\n",
      "set=0 Current iteration=2400,set=0, loss=0.3942987501328236\n",
      "set=0 Current iteration=2500,set=0, loss=0.39404025417158933\n",
      "set=0 Current iteration=2600,set=0, loss=0.39380577216582235\n",
      "set=0 Current iteration=2700,set=0, loss=0.3935928902382423\n",
      "set=0 Current iteration=2800,set=0, loss=0.39339946670195136\n",
      "set=0 Current iteration=2900,set=0, loss=0.39322358680161373\n",
      "set=0 Current iteration=3000,set=0, loss=0.39306353310332076\n",
      "set=0 Current iteration=3100,set=0, loss=0.39291776605104645\n",
      "set=0 Current iteration=3200,set=0, loss=0.392784907310689\n",
      "set=0 Current iteration=3300,set=0, loss=0.39266372321616944\n",
      "set=0 Current iteration=3400,set=0, loss=0.3925531088392649\n",
      "set=1 Current iteration=0,set=1, loss=0.6822895107452385\n",
      "set=1 Current iteration=100,set=1, loss=0.5595479566289464\n",
      "set=1 Current iteration=200,set=1, loss=0.5515477190992877\n",
      "set=1 Current iteration=300,set=1, loss=0.5478250860224229\n",
      "set=1 Current iteration=400,set=1, loss=0.5455567528125252\n",
      "set=1 Current iteration=500,set=1, loss=0.5440723366630376\n",
      "set=1 Current iteration=600,set=1, loss=0.5430589277772481\n",
      "set=1 Current iteration=700,set=1, loss=0.5423444785126258\n",
      "set=1 Current iteration=800,set=1, loss=0.5418276140219351\n",
      "set=1 Current iteration=900,set=1, loss=0.5414456354921815\n",
      "set=1 Current iteration=1000,set=1, loss=0.5411582341328544\n",
      "set=1 Current iteration=1100,set=1, loss=0.5409386561238196\n",
      "set=1 Current iteration=1200,set=1, loss=0.5407686639255388\n",
      "set=1 Current iteration=1300,set=1, loss=0.5406355392840066\n",
      "set=2 Current iteration=0,set=2, loss=0.6733096225154588\n",
      "set=2 Current iteration=100,set=2, loss=0.5394131964056024\n",
      "set=2 Current iteration=200,set=2, loss=0.5307952662103058\n",
      "set=2 Current iteration=300,set=2, loss=0.5260259143243834\n",
      "set=2 Current iteration=400,set=2, loss=0.5230138287273728\n",
      "set=2 Current iteration=500,set=2, loss=0.5209742533856461\n",
      "set=2 Current iteration=600,set=2, loss=0.5195242710803724\n",
      "set=2 Current iteration=700,set=2, loss=0.5184549415315829\n",
      "set=2 Current iteration=800,set=2, loss=0.5176436961124228\n",
      "set=2 Current iteration=900,set=2, loss=0.5170144929521637\n",
      "set=2 Current iteration=1000,set=2, loss=0.5165178887394469\n",
      "set=2 Current iteration=1100,set=2, loss=0.5161204259219423\n",
      "set=2 Current iteration=1200,set=2, loss=0.5157986854281522\n",
      "set=2 Current iteration=1300,set=2, loss=0.5155357997537848\n",
      "set=2 Current iteration=1400,set=2, loss=0.5153193239067213\n",
      "set=2 Current iteration=1500,set=2, loss=0.5151398873365653\n",
      "set=2 Current iteration=1600,set=2, loss=0.5149903121659533\n",
      "set=2 Current iteration=1700,set=2, loss=0.5148650194205733\n",
      "set=3 Current iteration=0,set=3, loss=0.6839045527057792\n",
      "set=3 Current iteration=100,set=3, loss=0.5437065391130964\n",
      "set=3 Current iteration=200,set=3, loss=0.5359810998725956\n",
      "set=3 Current iteration=300,set=3, loss=0.5329490834326931\n",
      "set=3 Current iteration=400,set=3, loss=0.5310732777141465\n",
      "set=3 Current iteration=500,set=3, loss=0.5297817053926819\n",
      "set=3 Current iteration=600,set=3, loss=0.528854648979769\n",
      "set=3 Current iteration=700,set=3, loss=0.5281729292589664\n",
      "set=3 Current iteration=800,set=3, loss=0.5276630211159257\n",
      "set=3 Current iteration=900,set=3, loss=0.5272765660908819\n",
      "set=3 Current iteration=1000,set=3, loss=0.5269804634512358\n",
      "set=3 Current iteration=1100,set=3, loss=0.526751424425525\n",
      "set=3 Current iteration=1200,set=3, loss=0.5265727321677478\n",
      "set=3 Current iteration=1300,set=3, loss=0.5264322024624639\n",
      "set=4 Current iteration=0,set=4, loss=0.6775659407702838\n",
      "set=4 Current iteration=100,set=4, loss=0.44341892942343564\n",
      "set=4 Current iteration=200,set=4, loss=0.42781606373445424\n",
      "set=4 Current iteration=300,set=4, loss=0.42125709141853057\n",
      "set=4 Current iteration=400,set=4, loss=0.4167264480264249\n",
      "set=4 Current iteration=500,set=4, loss=0.41317417039948345\n",
      "set=4 Current iteration=600,set=4, loss=0.41027917363837846\n",
      "set=4 Current iteration=700,set=4, loss=0.407876572074356\n",
      "set=4 Current iteration=800,set=4, loss=0.40585798264838924\n",
      "set=4 Current iteration=900,set=4, loss=0.4041453427670706\n",
      "set=4 Current iteration=1000,set=4, loss=0.40268024727905466\n",
      "set=4 Current iteration=1100,set=4, loss=0.4014180122457823\n",
      "set=4 Current iteration=1200,set=4, loss=0.4003238536304663\n",
      "set=4 Current iteration=1300,set=4, loss=0.3993702836857223\n",
      "set=4 Current iteration=1400,set=4, loss=0.398535290662196\n",
      "set=4 Current iteration=1500,set=4, loss=0.39780104267321054\n",
      "set=4 Current iteration=1600,set=4, loss=0.39715294759669323\n",
      "set=4 Current iteration=1700,set=4, loss=0.3965789579944277\n",
      "set=4 Current iteration=1800,set=4, loss=0.3960690478776561\n",
      "set=4 Current iteration=1900,set=4, loss=0.39561481281827715\n",
      "set=4 Current iteration=2000,set=4, loss=0.3952091605733163\n",
      "set=4 Current iteration=2100,set=4, loss=0.39484606934049904\n",
      "set=4 Current iteration=2200,set=4, loss=0.39452039726616384\n",
      "set=4 Current iteration=2300,set=4, loss=0.3942277312585124\n",
      "set=4 Current iteration=2400,set=4, loss=0.3939642663025543\n",
      "set=4 Current iteration=2500,set=4, loss=0.39372670877791\n",
      "set=4 Current iteration=2600,set=4, loss=0.3935121989979851\n",
      "set=4 Current iteration=2700,set=4, loss=0.39331824938730453\n",
      "set=4 Current iteration=2800,set=4, loss=0.39314269518475725\n",
      "set=4 Current iteration=2900,set=4, loss=0.3929836537495686\n",
      "set=4 Current iteration=3000,set=4, loss=0.3928394862870449\n",
      "set=4 Current iteration=3100,set=4, loss=0.3927087551008241\n",
      "set=4 Current iteration=3200,set=4, loss=0.39259017710400046\n",
      "set=4 Current iteration=3300,set=4, loss=0.3924825870886703\n",
      "set=5 Current iteration=0,set=5, loss=0.6830249895156275\n",
      "set=5 Current iteration=100,set=5, loss=0.5607114238286759\n",
      "set=5 Current iteration=200,set=5, loss=0.5527619134164855\n",
      "set=5 Current iteration=300,set=5, loss=0.5490746056069928\n",
      "set=5 Current iteration=400,set=5, loss=0.5468099345121673\n",
      "set=5 Current iteration=500,set=5, loss=0.5453140956883344\n",
      "set=5 Current iteration=600,set=5, loss=0.544282582107523\n",
      "set=5 Current iteration=700,set=5, loss=0.543547562861249\n",
      "set=5 Current iteration=800,set=5, loss=0.5430099045640913\n",
      "set=5 Current iteration=900,set=5, loss=0.5426081202174147\n",
      "set=5 Current iteration=1000,set=5, loss=0.5423025254611329\n",
      "set=5 Current iteration=1100,set=5, loss=0.5420666381887511\n",
      "set=5 Current iteration=1200,set=5, loss=0.5418822785082199\n",
      "set=5 Current iteration=1300,set=5, loss=0.5417366595470744\n",
      "set=5 Current iteration=1400,set=5, loss=0.5416205972227185\n",
      "set=6 Current iteration=0,set=6, loss=0.6740446890205017\n",
      "set=6 Current iteration=100,set=6, loss=0.540699462007101\n",
      "set=6 Current iteration=200,set=6, loss=0.5321446921671558\n",
      "set=6 Current iteration=300,set=6, loss=0.52746325596675\n",
      "set=6 Current iteration=400,set=6, loss=0.5244949478118327\n",
      "set=6 Current iteration=500,set=6, loss=0.5224768996461737\n",
      "set=6 Current iteration=600,set=6, loss=0.5210370046925238\n",
      "set=6 Current iteration=700,set=6, loss=0.5199716172041041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set=6 Current iteration=800,set=6, loss=0.519160784826803\n",
      "set=6 Current iteration=900,set=6, loss=0.51852982744917\n",
      "set=6 Current iteration=1000,set=6, loss=0.5180301135857194\n",
      "set=6 Current iteration=1100,set=6, loss=0.517628733987101\n",
      "set=6 Current iteration=1200,set=6, loss=0.5173026598357782\n",
      "set=6 Current iteration=1300,set=6, loss=0.5170353008655324\n",
      "set=6 Current iteration=1400,set=6, loss=0.5168144035461005\n",
      "set=6 Current iteration=1500,set=6, loss=0.5166307238736993\n",
      "set=6 Current iteration=1600,set=6, loss=0.5164771624380179\n",
      "set=6 Current iteration=1700,set=6, loss=0.5163481839883266\n",
      "set=6 Current iteration=1800,set=6, loss=0.5162394173445031\n",
      "set=7 Current iteration=0,set=7, loss=0.6842676826862074\n",
      "set=7 Current iteration=100,set=7, loss=0.5458683678702808\n",
      "set=7 Current iteration=200,set=7, loss=0.5381030531952116\n",
      "set=7 Current iteration=300,set=7, loss=0.5350853761408234\n",
      "set=7 Current iteration=400,set=7, loss=0.533235238059444\n",
      "set=7 Current iteration=500,set=7, loss=0.5319706244390818\n",
      "set=7 Current iteration=600,set=7, loss=0.5310675515373091\n",
      "set=7 Current iteration=700,set=7, loss=0.5304052636388914\n",
      "set=7 Current iteration=800,set=7, loss=0.5299102859864552\n",
      "set=7 Current iteration=900,set=7, loss=0.5295349446440615\n",
      "set=7 Current iteration=1000,set=7, loss=0.5292469347852241\n",
      "set=7 Current iteration=1100,set=7, loss=0.5290236794894402\n",
      "set=7 Current iteration=1200,set=7, loss=0.5288490410399446\n",
      "set=7 Current iteration=1300,set=7, loss=0.5287112866510119\n"
     ]
    }
   ],
   "source": [
    "# Observing LR\n",
    "w_final=[]\n",
    "for i in range(8):\n",
    "    max_iter = 4000\n",
    "    threshold = 1e-6\n",
    "    losses = []\n",
    "    tx_i = tx_full[i]\n",
    "    tx_i=standardize(tx_i)\n",
    "    y_i = (y_full[i]+1)/2\n",
    "    w_l = np.zeros(build_poly(tx_full[i],1).shape[1],)\n",
    "    #Dividing the gamma for the number of samples (it is the same as dividing the gradient)\n",
    "    gamma=(1e-1)/len(y_i)\n",
    "    for iter in range(max_iter):\n",
    "        w_l, loss_l = logistic_regression(y_i,build_poly(tx_i,1), w_l, 1, gamma,mode='GD')\n",
    "        #dividing the loss for the number of samples\n",
    "        loss_l=loss_l/len(y_i)\n",
    "        if iter % 100 == 0:\n",
    "            print(\"set={i} Current iteration={iter},set={i}, loss={l}\".format(i=i,iter=iter, l=loss_l))\n",
    "             #converge criterion\n",
    "        losses.append(loss_l)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    w_final.append(w_l)\n",
    "#visualitation\n",
    "#print('for {}, iteration {} w: {}, loss: {}'.format(i,iter,w_l,loss_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.02795223,  0.25527513, -0.38023046, -0.44671939,  0.53818345,\n",
       "       -0.3173693 ,  0.61231853,  0.08435523,  0.55453496, -0.00776721,\n",
       "       -0.05334809, -0.4503705 ,  0.26252371,  0.26490822,  0.30989789,\n",
       "       -0.03427286, -0.00147702,  0.51932606, -0.00753477,  0.00411351,\n",
       "        0.14747072,  0.02058241, -0.18124601, -0.33660195, -0.0014882 ,\n",
       "       -0.00396532, -0.06499279,  0.01742307,  0.007029  , -0.27387289])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_final[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from proj1_helpers import *\n",
    "#DATA_TEST_PATH = '../data/test.csv' \n",
    "# test_y, test_X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql2-oUuA4ynz"
   },
   "source": [
    "## Generate predictions using only features with no errrors throughought\n",
    "\n",
    "This enables us to use some of the methods from the course directly, without having to adjust some of the functionnality to account for the fact that a lot of errors are in the dataset. First let us see which features from the test dataset are error free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this figured out we can now extract the valid columns from test and train data, do some training and testing on data, then generate answers for the test data and submit to aicrowd !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkMBGpay4pjh"
   },
   "source": [
    "# Save prediction ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i--OcRst4pjh"
   },
   "outputs": [],
   "source": [
    "#DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "#_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "grQDhIJ84pjk"
   },
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "#create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
